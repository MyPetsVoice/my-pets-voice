"""
CSV ì•½í’ˆ ë°ì´í„°ë¥¼ RAG ì‹œìŠ¤í…œìš©ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ëª¨ë“ˆ
ëŒ€ìš©ëŸ‰ CSV íŒŒì¼ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ë²¡í„°í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
"""

import pandas as pd
import json
import re
from pathlib import Path
from typing import List, Dict, Any, Iterator, Optional
from dataclasses import dataclass
import ast

@dataclass
class MedicationDocument:
    """ì•½í’ˆ ì •ë³´ ë¬¸ì„œ êµ¬ì¡°"""
    product_name: str
    content: str
    metadata: Dict[str, Any]
    chunk_id: str
    keywords: List[str]

class CSVMedicationProcessor:
    """CSV ì•½í’ˆ ë°ì´í„° ì²˜ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, batch_size: int = 1000, max_content_length: int = 2000):
        self.batch_size = batch_size
        self.max_content_length = max_content_length
    
    def process_csv_files(self, csv_files: List[str]) -> Iterator[List[MedicationDocument]]:
        """ì—¬ëŸ¬ CSV íŒŒì¼ì„ ë°°ì¹˜ ì²˜ë¦¬"""
        for csv_file in csv_files:
            print(f"ì²˜ë¦¬ ì¤‘: {csv_file}")
            yield from self.process_single_csv(csv_file)
    
    def process_single_csv(self, csv_file: str) -> Iterator[List[MedicationDocument]]:
        """ë‹¨ì¼ CSV íŒŒì¼ì„ ë°°ì¹˜ë³„ë¡œ ì²˜ë¦¬"""
        try:
            # CSV íŒŒì¼ì„ ì²­í¬ ë‹¨ìœ„ë¡œ ì½ê¸° (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
            chunk_reader = pd.read_csv(
                csv_file, 
                chunksize=self.batch_size,
                encoding='utf-8-sig',  # BOM ì²˜ë¦¬
                na_values=['', 'NaN', 'None'],
                keep_default_na=False
            )
            
            batch_num = 0
            for chunk in chunk_reader:
                batch_num += 1
                print(f"  ë°°ì¹˜ {batch_num} ì²˜ë¦¬ ì¤‘... ({len(chunk)}í–‰)")
                
                documents = []
                for _, row in chunk.iterrows():
                    try:
                        doc = self._convert_row_to_document(row, csv_file, batch_num)
                        if doc:
                            documents.append(doc)
                    except Exception as e:
                        print(f"    í–‰ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        continue
                
                if documents:
                    yield documents
                    
        except Exception as e:
            print(f"CSV íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ {csv_file}: {e}")
    
    def _convert_row_to_document(self, row: pd.Series, source_file: str, batch_num: int) -> Optional[MedicationDocument]:
        """CSV í–‰ì„ MedicationDocumentë¡œ ë³€í™˜"""
        try:
            # ì œí’ˆëª… ì¶”ì¶œ
            product_name = str(row.get('ì œí’ˆëª…', '')).strip()
            if not product_name or product_name == 'nan':
                return None
            
            # ê¸°ë³¸ì •ë³´ íŒŒì‹± (JSON í˜•íƒœì˜ ë¬¸ìì—´)
            basic_info = self._parse_basic_info(row.get('ê¸°ë³¸ì •ë³´', '{}'))
            
            # ì›ë£Œì•½í’ˆ íŒŒì‹±
            ingredients = self._parse_ingredients(row.get('ì›ë£Œì•½í’ˆ ë° ë¶„ëŸ‰', '[]'))
            
            # ì½˜í…ì¸  ìƒì„±
            content = self._build_structured_content(
                product_name, basic_info, ingredients, row
            )
            
            # ì½˜í…ì¸  ê¸¸ì´ ì œí•œ
            if len(content) > self.max_content_length:
                content = content[:self.max_content_length] + "..."
            
            # ë©”íƒ€ë°ì´í„° êµ¬ì„±
            metadata = self._build_metadata(basic_info, source_file, batch_num)
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self._extract_keywords(product_name, basic_info, row)
            
            # ì²­í¬ ID ìƒì„±
            chunk_id = f"med_{Path(source_file).stem}_{batch_num}_{product_name.replace(' ', '_')}"
            
            return MedicationDocument(
                product_name=product_name,
                content=content,
                metadata=metadata,
                chunk_id=chunk_id,
                keywords=keywords
            )
            
        except Exception as e:
            print(f"ë¬¸ì„œ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return None
    
    def _parse_basic_info(self, basic_info_str: str) -> Dict[str, str]:
        """ê¸°ë³¸ì •ë³´ JSON ë¬¸ìì—´ íŒŒì‹±"""
        try:
            if pd.isna(basic_info_str) or not basic_info_str.strip():
                return {}
            
            # JSON í˜•íƒœ ë¬¸ìì—´ì„ dictë¡œ ë³€í™˜
            basic_info = ast.literal_eval(basic_info_str)
            if isinstance(basic_info, dict):
                return basic_info
            else:
                return {}
        except:
            return {}
    
    def _parse_ingredients(self, ingredients_str: str) -> List[Dict[str, str]]:
        """ì›ë£Œì•½í’ˆ ë¦¬ìŠ¤íŠ¸ ë¬¸ìì—´ íŒŒì‹±"""
        try:
            if pd.isna(ingredients_str) or not ingredients_str.strip():
                return []
            
            # ë¦¬ìŠ¤íŠ¸ í˜•íƒœ ë¬¸ìì—´ì„ listë¡œ ë³€í™˜
            ingredients_list = ast.literal_eval(ingredients_str)
            
            parsed_ingredients = []
            for ingredient in ingredients_list:
                if isinstance(ingredient, list) and len(ingredient) >= 4:
                    parsed_ingredients.append({
                        'name': ingredient[1] if len(ingredient) > 1 else '',
                        'amount': ingredient[2] if len(ingredient) > 2 else '',
                        'unit': ingredient[3] if len(ingredient) > 3 else ''
                    })
            
            return parsed_ingredients
        except:
            return []
    
    def _build_structured_content(self, product_name: str, basic_info: Dict, 
                                 ingredients: List[Dict], row: pd.Series) -> str:
        """êµ¬ì¡°í™”ëœ ì½˜í…ì¸  ìƒì„±"""
        content_parts = []
        
        # ì œí’ˆ ê¸°ë³¸ ì •ë³´
        content_parts.append(f"ì œí’ˆëª…: {product_name}")
        
        if basic_info.get('ì œí’ˆ ì˜ë¬¸ëª…'):
            content_parts.append(f"ì˜ë¬¸ëª…: {basic_info['ì œí’ˆ ì˜ë¬¸ëª…']}")
        
        if basic_info.get('ì—…ì²´ëª…'):
            content_parts.append(f"ì œì¡°ì—…ì²´: {basic_info['ì—…ì²´ëª…']}")
        
        if basic_info.get('í—ˆê°€ì¼'):
            content_parts.append(f"í—ˆê°€ì¼: {basic_info['í—ˆê°€ì¼']}")
        
        if basic_info.get('ì„±ìƒ'):
            content_parts.append(f"ì„±ìƒ: {basic_info['ì„±ìƒ']}")
        
        # ì›ë£Œì•½í’ˆ ì •ë³´
        if ingredients:
            content_parts.append("\\nì£¼ìš” ì›ë£Œì•½í’ˆ:")
            for ingredient in ingredients[:10]:  # ìƒìœ„ 10ê°œë§Œ
                name = ingredient.get('name', '').strip()
                amount = ingredient.get('amount', '').strip()
                unit = ingredient.get('unit', '').strip()
                if name:
                    ingredient_info = f"â€¢ {name}"
                    if amount and amount != '-':
                        ingredient_info += f": {amount}"
                        if unit and unit != '-':
                            ingredient_info += f" {unit}"
                    content_parts.append(ingredient_info)
        
        # íš¨ëŠ¥íš¨ê³¼
        efficacy = str(row.get('íš¨ëŠ¥íš¨ê³¼', '')).strip()
        if efficacy and efficacy != 'nan':
            content_parts.append(f"\\níš¨ëŠ¥íš¨ê³¼: {efficacy}")
        
        # ìš©ë²•ìš©ëŸ‰
        dosage = str(row.get('ìš©ë²•ìš©ëŸ‰', '')).strip()
        if dosage and dosage != 'nan':
            # ìš©ë²•ìš©ëŸ‰ ì •ë¦¬ (ë„ˆë¬´ ê¸¸ë©´ ì¶•ì•½)
            if len(dosage) > 500:
                dosage = dosage[:500] + "..."
            content_parts.append(f"\\nìš©ë²•ìš©ëŸ‰:\\n{dosage}")
        
        # ì£¼ì˜ì‚¬í•­
        warnings = str(row.get('ì£¼ì˜ì‚¬í•­', '')).strip()
        if warnings and warnings != 'nan':
            # ì£¼ì˜ì‚¬í•­ ì •ë¦¬ (ë„ˆë¬´ ê¸¸ë©´ ì¶•ì•½)
            if len(warnings) > 300:
                warnings = warnings[:300] + "..."
            content_parts.append(f"\\nì£¼ì˜ì‚¬í•­: {warnings}")
        
        return "\\n".join(content_parts)
    
    def _build_metadata(self, basic_info: Dict, source_file: str, batch_num: int) -> Dict[str, Any]:
        """ë©”íƒ€ë°ì´í„° êµ¬ì„±"""
        metadata = {
            'source_file': Path(source_file).name,
            'source_type': 'csv_medication',
            'batch_number': batch_num,
            'company': basic_info.get('ì—…ì²´ëª…', ''),
            'approval_date': basic_info.get('í—ˆê°€ì¼', ''),
            'product_type': basic_info.get('í’ˆëª©ì •ë³´', ''),
            'approval_status': basic_info.get('ì—…ìƒíƒœ', ''),
            'manufacturing_type': basic_info.get('ì œì¡°/ìˆ˜ì…êµ¬ë¶„', ''),
        }
        
        # ë¹ˆ ê°’ ì œê±°
        metadata = {k: v for k, v in metadata.items() if v and v != 'nan'}
        
        return metadata
    
    def _extract_keywords(self, product_name: str, basic_info: Dict, row: pd.Series) -> List[str]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = []
        
        # ì œí’ˆëª…ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords.append(product_name)
        
        # ì˜ë¬¸ëª…
        if basic_info.get('ì œí’ˆ ì˜ë¬¸ëª…'):
            keywords.append(basic_info['ì œí’ˆ ì˜ë¬¸ëª…'])
        
        # ì—…ì²´ëª…
        if basic_info.get('ì—…ì²´ëª…'):
            keywords.append(basic_info['ì—…ì²´ëª…'])
        
        # íš¨ëŠ¥íš¨ê³¼ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        efficacy = str(row.get('íš¨ëŠ¥íš¨ê³¼', '')).strip()
        if efficacy and efficacy != 'nan':
            # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ íš¨ëŠ¥ë“¤ ì¶”ì¶œ
            efficacy_keywords = [kw.strip() for kw in efficacy.split(',') if kw.strip()]
            keywords.extend(efficacy_keywords[:5])  # ìƒìœ„ 5ê°œë§Œ
        
        # ì¤‘ë³µ ì œê±°í•˜ê³  ì •ë¦¬
        unique_keywords = []
        for kw in keywords:
            if kw and kw not in unique_keywords and len(kw.strip()) > 1:
                unique_keywords.append(kw.strip())
        
        return unique_keywords[:10]  # ìµœëŒ€ 10ê°œ

class MedicationRAGProcessor:
    """ì•½í’ˆ ë°ì´í„°ë¥¼ RAGìš©ìœ¼ë¡œ ìµœì¢… ì²˜ë¦¬"""
    
    @staticmethod
    def prepare_for_vectorization(documents: List[MedicationDocument]) -> List[Dict[str, Any]]:
        """ë²¡í„°í™”ë¥¼ ìœ„í•œ ë°ì´í„° ì¤€ë¹„"""
        vector_docs = []
        
        for doc in documents:
            # ì»¨í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ í…ìŠ¤íŠ¸ êµ¬ì„±
            context_text = MedicationRAGProcessor._build_context_text(doc)
            
            vector_doc = {
                'id': doc.chunk_id,
                'text': context_text,
                'metadata': {
                    **doc.metadata,
                    'product_name': doc.product_name,
                    'keywords': doc.keywords,
                    'content_type': 'medication_info'
                }
            }
            vector_docs.append(vector_doc)
        
        return vector_docs
    
    @staticmethod
    def _build_context_text(doc: MedicationDocument) -> str:
        """ì»¨í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ í…ìŠ¤íŠ¸ êµ¬ì„±"""
        context_parts = []
        
        # ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸
        context_parts.append(f"ë¬¸ì„œìœ í˜•: ë™ë¬¼ìš©ì˜ì•½í’ˆ")
        context_parts.append(f"ì œí’ˆëª…: {doc.product_name}")
        
        # ë©”íƒ€ë°ì´í„°ì—ì„œ ì¤‘ìš” ì •ë³´ ì¶”ì¶œ
        if doc.metadata.get('company'):
            context_parts.append(f"ì œì¡°ì—…ì²´: {doc.metadata['company']}")
        
        if doc.metadata.get('product_type'):
            context_parts.append(f"í’ˆëª©ì •ë³´: {doc.metadata['product_type']}")
        
        # í‚¤ì›Œë“œ
        if doc.keywords:
            context_parts.append(f"í‚¤ì›Œë“œ: {', '.join(doc.keywords[:5])}")
        
        # ì»¨í…ìŠ¤íŠ¸ì™€ ë‚´ìš© ê²°í•©
        context = ' | '.join(context_parts)
        return f"{context}\\n\\n{doc.content}"
    
    @staticmethod
    def save_processed_data(vector_docs: List[Dict], output_path: str):
        """ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ì €ì¥"""
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(vector_docs, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… {len(vector_docs)}ê°œ ë¬¸ì„œê°€ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ì‚¬ìš© ì˜ˆì œ ë° ë°°ì¹˜ ì²˜ë¦¬
class BatchMedicationProcessor:
    """ëŒ€ìš©ëŸ‰ CSV íŒŒì¼ì˜ ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ í´ë˜ìŠ¤"""
    
    def __init__(self, output_dir: str = "processed_medications"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
    
    def process_all_csv_files(self, csv_files: List[str]):
        """ëª¨ë“  CSV íŒŒì¼ì„ ë°°ì¹˜ ì²˜ë¦¬"""
        processor = CSVMedicationProcessor(batch_size=500)  # ë©”ëª¨ë¦¬ ì ˆì•½
        rag_processor = MedicationRAGProcessor()
        
        total_processed = 0
        
        for csv_file in csv_files:
            file_processed = 0
            output_parts = []
            
            # íŒŒì¼ë³„ ë°°ì¹˜ ì²˜ë¦¬
            for batch_documents in processor.process_single_csv(csv_file):
                # RAGìš© ë°ì´í„° ì¤€ë¹„
                vector_docs = rag_processor.prepare_for_vectorization(batch_documents)
                output_parts.extend(vector_docs)
                
                file_processed += len(batch_documents)
                
                # ì¤‘ê°„ ì €ì¥ (ë©”ëª¨ë¦¬ ê´€ë¦¬)
                if len(output_parts) >= 2000:  # 2000ê°œì”© ì €ì¥
                    self._save_batch(output_parts, csv_file, len(output_parts))
                    output_parts = []
            
            # ë‚¨ì€ ë°ì´í„° ì €ì¥
            if output_parts:
                self._save_batch(output_parts, csv_file, file_processed)
            
            total_processed += file_processed
            print(f"âœ… {csv_file}: {file_processed}ê°œ ì²˜ë¦¬ ì™„ë£Œ")
        
        print(f"ğŸ‰ ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ: {total_processed}ê°œ ì•½í’ˆ ì •ë³´")
    
    def _save_batch(self, data: List[Dict], source_file: str, batch_id: int):
        """ë°°ì¹˜ ë°ì´í„° ì €ì¥"""
        filename = f"{Path(source_file).stem}_batch_{batch_id}.json"
        output_path = self.output_dir / filename
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

# ì‹¤í–‰ ì˜ˆì œ
if __name__ == "__main__":
    # CSV íŒŒì¼ ëª©ë¡
    csv_files = [
        "medications/medicine_data_1p~500p.csv",
        "medications/medicine_data_fixed(401-730).csv", 
        "medications/medicine_data_fixed731.csv"
    ]
    
    # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰
    batch_processor = BatchMedicationProcessor("processed_medications")
    batch_processor.process_all_csv_files(csv_files)
    
    print("\\nğŸ“Š ì²˜ë¦¬ í†µê³„:")
    print(f"- ì›ë³¸ íŒŒì¼: {len(csv_files)}ê°œ")
    print(f"- ì˜ˆìƒ ì²˜ë¦¬ëŸ‰: ~40ë§Œ ê°œ ì•½í’ˆ ì •ë³´")
    print(f"- ì¶œë ¥ í˜•ì‹: êµ¬ì¡°í™”ëœ JSON (RAG ìµœì í™”)")
    print(f"- ì €ì¥ ìœ„ì¹˜: processed_medications/ í´ë”")