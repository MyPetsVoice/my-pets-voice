# MyPetsVoice RAG ì‹œìŠ¤í…œ ë¬¸ì„œ ì²˜ë¦¬ ê°€ì´ë“œ

## ğŸ“š ê°œìš”

ì´ ê°€ì´ë“œëŠ” MyPetsVoice RAG ì‹œìŠ¤í…œì—ì„œ ë°˜ë ¤ë™ë¬¼ ê´€ë ¨ ë¬¸ì„œë“¤ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³  ë²¡í„° DBì— ì €ì¥í•˜ê¸° ìœ„í•œ ì™„ì „í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ—‚ï¸ í˜„ì¬ ë°ì´í„° êµ¬ì¡° ë¶„ì„

### ë°ì´í„° í˜„í™©
```
data/rag data/
â”œâ”€â”€ ğŸ“ breeds/dog_breed/           # ê²¬ì¢…ë³„ ê±´ê°•ê´€ë¦¬ (JSON, MD)
â”œâ”€â”€ ğŸ“ disease/                    # ì§ˆë³‘ ì •ë³´ (JSON, MD)  
â”œâ”€â”€ ğŸ“ emergency/                  # ì‘ê¸‰ìƒí™© ê°€ì´ë“œ (MD)
â”œâ”€â”€ ğŸ“ general_knowledge/          # ì¼ë°˜ ì§€ì‹ (MD)
â”œâ”€â”€ ğŸ“ healthcare/                 # ê±´ê°•ê´€ë¦¬ ê°€ì´ë“œ (MD)
â”œâ”€â”€ ğŸ“ medications/                # ì˜ì•½í’ˆ ì›ë³¸ ë°ì´í„° (CSV)
â”œâ”€â”€ ğŸ“ processed_medications/      # ì²˜ë¦¬ëœ ì˜ì•½í’ˆ (JSON)
â”œâ”€â”€ ğŸ“„ ë™ë¬¼ë“±ë¡ì œ.pdf              # ì •ë¶€ ì •ì±… ë¬¸ì„œ
â”œâ”€â”€ ğŸ“„ ë°˜ë ¤ìƒí™œ ê¸¸ì¡ì´.pdf         # ë°˜ë ¤ìƒí™œ ê°€ì´ë“œ
â””â”€â”€ ğŸ“„ ì†Œìœ ìì¤€ìˆ˜ì‚¬í•­.pdf          # ì†Œìœ ì ì˜ë¬´ì‚¬í•­
```

### íŒŒì¼ í˜•ì‹ë³„ ë¶„í¬
- **JSON**: 8ê°œ íŒŒì¼ (í’ˆì¢…ë³„ ì •ë³´, ì§ˆë³‘ ì •ë³´, ì˜ì•½í’ˆ ë°ì´í„°)
- **Markdown**: 6ê°œ íŒŒì¼ (ê°€ì´ë“œ, ì‘ê¸‰ìƒí™©, ê±´ê°•ê´€ë¦¬)
- **CSV**: 3ê°œ íŒŒì¼ (ì˜ì•½í’ˆ ì›ë³¸ ë°ì´í„°)
- **PDF**: 3ê°œ íŒŒì¼ (ì •ë¶€ ì •ì±… ë¬¸ì„œ)

## ğŸ¯ Chunking ì „ëµ

### 1. í†µí•© í…ìŠ¤íŠ¸ ë³€í™˜ ì „ëµ

ëª¨ë“  ë¬¸ì„œë¥¼ ì¼ê´€ëœ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ê²€ìƒ‰ íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤.

#### ë³€í™˜ í˜•ì‹ ì˜ˆì‹œ

**JSON â†’ í†µí•© í…ìŠ¤íŠ¸**
```
ë¬¸ì„œ: maltese_healthcare | ìœ í˜•: dog_health | ID: maltese_puppy_healthcare | í‚¤ì›Œë“œ: ë§í‹°ì¦ˆ, í¼í”¼, ì˜ˆë°©ì ‘ì¢…

ë§í‹°ì¦ˆ í¼í”¼ í”Œëœ (~1ì„¸ ë¯¸ë§Œ) ê±´ê°•ê´€ë¦¬:

ì˜ˆë°©ì ‘ì¢… ì¼ì •:
â€¢ 1ì°¨ (ì¢…í•©ë°±ì‹ +ì½”ë¡œë‚˜ ì¥ì—¼): ìƒí›„ 6-8ì£¼
â€¢ 2ì°¨ (ì¢…í•©ë°±ì‹ +ì½”ë¡œë‚˜ ì¥ì—¼): ìƒí›„ 8-10ì£¼
...
```

**Markdown â†’ í†µí•© í…ìŠ¤íŠ¸**
```
ë¬¸ì„œ: ë°˜ë ¤ë™ë¬¼_ì¬ë‚œ_ëŒ€ì‘_ê°€ì´ë“œë¼ì¸ | ìœ í˜•: emergency | ìƒìœ„ ì„¹ì…˜: ì¬ë‚œ ë°œìƒ ì „ ì¤€ë¹„ì‚¬í•­ | ì œëª©: ëŒ€í”¼ ê³„íš ì„¸ìš°ê¸° | í‚¤ì›Œë“œ: ëŒ€í”¼ ê³„íš, ì¬ë‚œ ì¤€ë¹„

## 1. ëŒ€í”¼ ê³„íš ì„¸ìš°ê¸°
- **ëŒ€í”¼ì‹œì„¤ íŒŒì•…**: ì¬ë‚œ ë°œìƒ ì‹œ ë°˜ë ¤ë™ë¬¼ê³¼ í•¨ê»˜ ì…ì¥í•  ìˆ˜ ìˆëŠ”...
```

### 2. íŒŒì¼ íƒ€ì…ë³„ ì²˜ë¦¬ ì „ëµ

#### JSON íŒŒì¼
- **ì²­í‚¹ ë‹¨ìœ„**: ê° JSON ê°ì²´ì˜ `content` í•„ë“œ
- **í¬ê¸° ì œí•œ**: 512-1024 í† í°
- **ì»¨í…ìŠ¤íŠ¸ ë³´ì¡´**: `id`, `type` ë©”íƒ€ë°ì´í„° í™œìš©
- **í‚¤ì›Œë“œ ì¶”ì¶œ**: ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ í‚¤ì›Œë“œ ì„¹ì…˜ íŒŒì‹±

#### Markdown íŒŒì¼
- **ì²­í‚¹ ë‹¨ìœ„**: í—¤ë”(`##`, `###`) ê¸°ì¤€ ì„¹ì…˜
- **ê³„ì¸µ êµ¬ì¡°**: ìƒìœ„ í—¤ë” ì •ë³´ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ í¬í•¨
- **ëŒ€í˜• ì„¹ì…˜**: ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì¶”ê°€ ë¶„í•  (512+ í† í° ì‹œ)
- **ë©”íƒ€ë°ì´í„°**: ì œëª©, ë ˆë²¨, ìƒìœ„ ì„¹ì…˜ ì •ë³´

#### CSV íŒŒì¼ (ì˜ì•½í’ˆ ë°ì´í„°)
- **ì „ì²˜ë¦¬**: ì´ë¯¸ JSONìœ¼ë¡œ ë³€í™˜ëœ ë°ì´í„° í™œìš©
- **êµ¬ì¡°í™”**: ì˜ì•½í’ˆë³„ë¡œ ê°œë³„ ì²­í¬ ìƒì„±
- **ë©”íƒ€ë°ì´í„°**: ì œí’ˆëª…, ì œì¡°ì—…ì²´, í—ˆê°€ì¼, ì¶œì²˜ URL

## ğŸ—ï¸ ë©”íƒ€ë°ì´í„° ìŠ¤í‚¤ë§ˆ

### í•„ìˆ˜ ë©”íƒ€ë°ì´í„°
```json
{
  "source_file": "maltese_healthcare.json",
  "source_type": "json",
  "chunk_type": "dog_health", 
  "file_path": "/data/rag data/breeds/dog_breed/maltese_healthcare.json",
  "processed_date": "2024-09-05T14:30:00.000Z"
}
```

### ì„ íƒì  ë©”íƒ€ë°ì´í„°
```json
{
  "id": "maltese_puppy_healthcare",
  "title": "ë§í‹°ì¦ˆ í¼í”¼ ê±´ê°•ê´€ë¦¬",
  "keywords": ["ë§í‹°ì¦ˆ", "í¼í”¼", "ì˜ˆë°©ì ‘ì¢…"],
  "source_url": "https://medi.qia.go.kr/searchMedicine",
  "publisher": "ë†ë¦¼ì¶•ì‚°ê²€ì—­ë³¸ë¶€",
  "token_count": 256,
  "quality_score": 0.9
}
```

### Chunk íƒ€ì… ë¶„ë¥˜
- `dog_health`: ë°˜ë ¤ê²¬ ê±´ê°•ê´€ë¦¬
- `cat_health`: ë°˜ë ¤ë¬˜ ê±´ê°•ê´€ë¦¬  
- `medicine`: ë™ë¬¼ìš© ì˜ì•½í’ˆ ì •ë³´
- `disease`: ì§ˆë³‘ ì •ë³´
- `emergency`: ì‘ê¸‰ìƒí™© ëŒ€ì‘
- `registration`: ë™ë¬¼ë“±ë¡ì œ
- `lifestyle`: ë°˜ë ¤ìƒí™œ ê°€ì´ë“œ
- `owner_obligation`: ì†Œìœ ì ì˜ë¬´ì‚¬í•­

## ğŸ’» êµ¬í˜„ ê°€ì´ë“œ

### 1. í™˜ê²½ ì„¤ì •

```bash
# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install chromadb sentence-transformers pandas markdown

# í”„ë¡œì íŠ¸ êµ¬ì¡°
project/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ rag data/          # ì›ë³¸ ë°ì´í„°
â”œâ”€â”€ processed/             # ì²˜ë¦¬ëœ ë°ì´í„°
â”œâ”€â”€ vector_db/            # ë²¡í„° DB ì €ì¥ì†Œ
â””â”€â”€ scripts/
    â”œâ”€â”€ rag_chunking_strategy.py
    â””â”€â”€ vector_db_manager.py
```

### 2. ê¸°ë³¸ ì‚¬ìš©ë²•

```python
from rag_chunking_strategy import UnifiedDocumentProcessor, RAGChunkProcessor

# ë¬¸ì„œ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
processor = UnifiedDocumentProcessor(
    chunk_size=512,
    overlap=50,
    min_chunk_size=100
)

# ì²­í¬ í›„ì²˜ë¦¬ê¸°
chunk_processor = RAGChunkProcessor()

# ëª¨ë“  ë¬¸ì„œ ì²˜ë¦¬
all_chunks = []
for file_path in document_files:
    chunks = processor.process_document(file_path)
    all_chunks.extend(chunks)

# ì¤‘ë³µ ì œê±° ë° í’ˆì§ˆ í–¥ìƒ
unique_chunks = chunk_processor.remove_duplicates(all_chunks)
vector_data = chunk_processor.prepare_for_vectorization(unique_chunks)
```

### 3. ë²¡í„° DB ì €ì¥

```python
import chromadb
from sentence_transformers import SentenceTransformer

# ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = chromadb.PersistentClient(path="./vector_db")
collection = client.get_or_create_collection("pet_healthcare")

# ì„ë² ë”© ëª¨ë¸ (í•œêµ­ì–´ ì§€ì›)
model = SentenceTransformer('jhgan/ko-sroberta-multitask')

# ë²¡í„° DBì— ì €ì¥
for chunk in vector_data:
    embedding = model.encode(chunk['text'])
    
    collection.add(
        ids=[chunk['id']],
        embeddings=[embedding.tolist()],
        documents=[chunk['text']],
        metadatas=[chunk['metadata']]
    )
```

## ğŸ” ê²€ìƒ‰ ìµœì í™” ì „ëµ

### 1. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰

```python
def hybrid_search(query: str, collection, n_results: int = 5):
    """í‚¤ì›Œë“œ + ë²¡í„° ìœ ì‚¬ë„ ê¸°ë°˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""
    
    # 1. ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰
    vector_results = collection.query(
        query_texts=[query],
        n_results=n_results * 2,  # ë” ë§ì€ í›„ë³´ ê°€ì ¸ì˜¤ê¸°
        include=['documents', 'metadatas', 'distances']
    )
    
    # 2. ë©”íƒ€ë°ì´í„° í•„í„°ë§
    filtered_results = []
    for doc, metadata, distance in zip(
        vector_results['documents'][0],
        vector_results['metadatas'][0], 
        vector_results['distances'][0]
    ):
        # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
        keyword_score = calculate_keyword_match(query, metadata.get('keywords', []))
        
        # ì¢…í•© ì ìˆ˜ = ë²¡í„° ìœ ì‚¬ë„ + í‚¤ì›Œë“œ ë§¤ì¹­
        final_score = (1 - distance) * 0.7 + keyword_score * 0.3
        
        filtered_results.append({
            'document': doc,
            'metadata': metadata,
            'score': final_score
        })
    
    # ì ìˆ˜ìˆœ ì •ë ¬ í›„ ìƒìœ„ ê²°ê³¼ ë°˜í™˜
    filtered_results.sort(key=lambda x: x['score'], reverse=True)
    return filtered_results[:n_results]
```

### 2. ì»¨í…ìŠ¤íŠ¸ ë³´ê°• ê²€ìƒ‰

```python
def contextual_search(query: str, collection):
    """ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ í™œìš©í•œ ê²€ìƒ‰"""
    
    results = hybrid_search(query, collection)
    
    # ê²°ê³¼ì— ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ê°€
    enriched_results = []
    for result in results:
        metadata = result['metadata']
        
        # ìƒìœ„ ì„¹ì…˜ ì •ë³´ ì¶”ê°€ (Markdown)
        context_info = ""
        if metadata.get('parent_titles'):
            context_path = ' > '.join(metadata['parent_titles'])
            context_info = f"ì„¹ì…˜: {context_path} > {metadata.get('title', '')}"
        
        # ì¶œì²˜ ì •ë³´ ì¶”ê°€
        source_info = f"ì¶œì²˜: {metadata.get('publisher', metadata.get('source_file', ''))}"
        
        enriched_results.append({
            'content': result['document'],
            'context': context_info,
            'source': source_info,
            'chunk_type': metadata.get('chunk_type'),
            'keywords': metadata.get('keywords', []),
            'score': result['score']
        })
    
    return enriched_results
```

## ğŸ“Š í’ˆì§ˆ ê´€ë¦¬

### 1. ìë™ í’ˆì§ˆ í‰ê°€

```python
def calculate_quality_score(chunk: DocumentChunk) -> float:
    """ì²­í¬ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
    score = 1.0
    
    # ê¸¸ì´ ê²€ì¦ (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ì²­í¬ í˜ë„í‹°)
    if len(chunk.text) < 50:
        score *= 0.5
    elif len(chunk.text) > 2000:
        score *= 0.8
    
    # í‚¤ì›Œë“œ ì¡´ì¬ ì—¬ë¶€
    if chunk.metadata.get('keywords'):
        score *= 1.1
    
    # ì¶œì²˜ ì •ë³´ ì™„ì„±ë„
    if chunk.metadata.get('source_url'):
        score *= 1.05
    
    # êµ¬ì¡°ì  ì •ë³´ ì™„ì„±ë„ (ì œëª©, ì„¹ì…˜ ë“±)
    if chunk.metadata.get('title'):
        score *= 1.05
    
    return min(score, 1.0)
```

### 2. ì¤‘ë³µ ì œê±° ì „ëµ

```python
def semantic_deduplication(chunks: List[DocumentChunk], threshold: float = 0.95) -> List[DocumentChunk]:
    """ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±°"""
    
    # ì„ë² ë”© ìƒì„±
    texts = [chunk.text for chunk in chunks]
    embeddings = model.encode(texts)
    
    # ìœ ì‚¬ë„ ë§¤íŠ¸ë¦­ìŠ¤ ê³„ì‚°
    similarity_matrix = cosine_similarity(embeddings)
    
    # ì¤‘ë³µ ì œê±°
    keep_indices = []
    for i, chunk in enumerate(chunks):
        is_duplicate = False
        for j in keep_indices:
            if similarity_matrix[i][j] > threshold:
                # í’ˆì§ˆì´ ë” ë†’ì€ ì²­í¬ ì„ íƒ
                if calculate_quality_score(chunk) <= calculate_quality_score(chunks[j]):
                    is_duplicate = True
                    break
        
        if not is_duplicate:
            keep_indices.append(i)
    
    return [chunks[i] for i in keep_indices]
```

## ğŸš€ ë°°í¬ ë° ëª¨ë‹ˆí„°ë§

### 1. ë°°ì¹˜ ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸

```python
# batch_process.py
import logging
from pathlib import Path
from rag_chunking_strategy import UnifiedDocumentProcessor

def batch_process_documents():
    """ëª¨ë“  ë¬¸ì„œë¥¼ ë°°ì¹˜ë¡œ ì²˜ë¦¬"""
    
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    
    processor = UnifiedDocumentProcessor()
    rag_data_path = Path("./data/rag data")
    
    # ì²˜ë¦¬í•  íŒŒì¼ ìˆ˜ì§‘
    files_to_process = []
    for pattern in ["**/*.json", "**/*.md", "**/*.txt"]:
        files_to_process.extend(rag_data_path.glob(pattern))
    
    logger.info(f"ì´ {len(files_to_process)}ê°œ íŒŒì¼ ì²˜ë¦¬ ì‹œì‘")
    
    processed_chunks = []
    failed_files = []
    
    for file_path in files_to_process:
        try:
            chunks = processor.process_document(file_path)
            processed_chunks.extend(chunks)
            logger.info(f"âœ… {file_path.name}: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
        
        except Exception as e:
            logger.error(f"âŒ {file_path.name}: {str(e)}")
            failed_files.append((file_path, str(e)))
    
    logger.info(f"ì²˜ë¦¬ ì™„ë£Œ: {len(processed_chunks)}ê°œ ì²­í¬ ìƒì„±")
    logger.info(f"ì‹¤íŒ¨í•œ íŒŒì¼: {len(failed_files)}ê°œ")
    
    return processed_chunks, failed_files

if __name__ == "__main__":
    chunks, failed = batch_process_documents()
```

### 2. ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ

```python
def generate_processing_report(chunks: List[DocumentChunk]) -> dict:
    """ì²˜ë¦¬ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
    
    report = {
        'total_chunks': len(chunks),
        'by_source_type': {},
        'by_chunk_type': {},
        'average_token_count': 0,
        'quality_distribution': {'high': 0, 'medium': 0, 'low': 0}
    }
    
    # í†µê³„ ê³„ì‚°
    token_counts = []
    for chunk in chunks:
        # ì†ŒìŠ¤ íƒ€ì…ë³„
        source_type = chunk.metadata.get('source_type', 'unknown')
        report['by_source_type'][source_type] = report['by_source_type'].get(source_type, 0) + 1
        
        # ì²­í¬ íƒ€ì…ë³„  
        chunk_type = chunk.metadata.get('chunk_type', 'unknown')
        report['by_chunk_type'][chunk_type] = report['by_chunk_type'].get(chunk_type, 0) + 1
        
        # í† í° ìˆ˜
        if chunk.token_count:
            token_counts.append(chunk.token_count)
        
        # í’ˆì§ˆ ë¶„í¬
        quality = chunk.metadata.get('quality_score', 0.5)
        if quality >= 0.8:
            report['quality_distribution']['high'] += 1
        elif quality >= 0.6:
            report['quality_distribution']['medium'] += 1
        else:
            report['quality_distribution']['low'] += 1
    
    if token_counts:
        report['average_token_count'] = sum(token_counts) / len(token_counts)
    
    return report
```

## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ì¼ë°˜ì ì¸ ë¬¸ì œë“¤

#### 1. JSON êµ¬ì¡° ë¶ˆì¼ì¹˜
**ë¬¸ì œ**: ì„œë¡œ ë‹¤ë¥¸ JSON ìŠ¤í‚¤ë§ˆ
**í•´ê²°ì±…**: `_extract_json_metadata()` í•¨ìˆ˜ì˜ flexible key handling í™œìš©

#### 2. í•œê¸€ ì¸ì½”ë”© ë¬¸ì œ  
**ë¬¸ì œ**: PDF íŒŒì¼ëª… ì¸ì‹ ì‹¤íŒ¨
**í•´ê²°ì±…**: UTF-8 ì¸ì½”ë”© ëª…ì‹œ, íŒŒì¼ëª… ì •ê·œí™”

#### 3. ì²­í¬ í¬ê¸° ë¶ˆê· í˜•
**ë¬¸ì œ**: ë„ˆë¬´ í¬ê±°ë‚˜ ì‘ì€ ì²­í¬
**í•´ê²°ì±…**: ë™ì  í¬ê¸° ì¡°ì •, `min_chunk_size` ì„¤ì •

#### 4. ì¤‘ë³µ ì½˜í…ì¸ 
**ë¬¸ì œ**: ê°™ì€ ë‚´ìš©ì˜ ì¤‘ë³µ ì €ì¥
**í•´ê²°ì±…**: `semantic_deduplication()` í•¨ìˆ˜ ì‚¬ìš©

### ì„±ëŠ¥ ìµœì í™”

```python
# ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ë¥¼ ìœ„í•œ ìµœì í™”
def process_large_dataset(file_paths: List[Path], batch_size: int = 100):
    """ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ë°°ì¹˜ ì²˜ë¦¬"""
    
    processor = UnifiedDocumentProcessor()
    
    for i in range(0, len(file_paths), batch_size):
        batch = file_paths[i:i + batch_size]
        
        batch_chunks = []
        for file_path in batch:
            chunks = processor.process_document(file_path)
            batch_chunks.extend(chunks)
        
        # ë°°ì¹˜ë³„ë¡œ ë²¡í„° DB ì €ì¥
        save_to_vector_db(batch_chunks)
        
        print(f"ë°°ì¹˜ {i//batch_size + 1}/{(len(file_paths)-1)//batch_size + 1} ì™„ë£Œ")
```

## ğŸ“‹ ì²´í¬ë¦¬ìŠ¤íŠ¸

### êµ¬í˜„ ì „ í™•ì¸ì‚¬í•­
- [ ] ëª¨ë“  ì˜ì¡´ì„± íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ
- [ ] ì›ë³¸ ë°ì´í„° ë°±ì—… ì™„ë£Œ  
- [ ] ë²¡í„° DB ì €ì¥ì†Œ ê²½ë¡œ ì„¤ì •
- [ ] ì„ë² ë”© ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ

### ì²˜ë¦¬ í›„ ê²€ì¦ì‚¬í•­
- [ ] ëª¨ë“  íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ í™•ì¸
- [ ] ì²­í¬ í’ˆì§ˆ ì ìˆ˜ ë¶„í¬ í™•ì¸
- [ ] ì¤‘ë³µ ì œê±° ê²°ê³¼ ê²€í† 
- [ ] ë©”íƒ€ë°ì´í„° ì™„ì„±ë„ ê²€ì¦
- [ ] ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ

---

ì´ ê°€ì´ë“œë¥¼ ë”°ë¼ êµ¬í˜„í•˜ë©´ MyPetsVoice RAG ì‹œìŠ¤í…œì—ì„œ ëª¨ë“  ë°˜ë ¤ë™ë¬¼ ê´€ë ¨ ë¬¸ì„œë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³  í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.